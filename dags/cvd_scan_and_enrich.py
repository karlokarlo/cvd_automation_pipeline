from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from datetime import datetime
import subprocess
from subprocess import CalledProcessError
import json
import requests
import re
import os


# Currently not used
def run_uncover_scan(**kwargs):
    # Set the path to the API key file - define the yaml api key file as described on uncover github readme page
    api_key_file_path = '/opt/airflow/config/uncover_api_keys.yaml'
    os.environ['UNCOVER_CONFIG'] = api_key_file_path

    # Get uncover search phrase from Airflow Variable
    uncover_search_phrase = Variable.get("uncover_search_phrase")
    divd_case_number = Variable.get("divd_case_number")

    # Running uncover with the provided search phrase
    shodan_command = f"uncover -s '{uncover_search_phrase}' -l 9999999 > data/{divd_case_number}/shodan-results.txt"
    censys_command = f"uncover -cs '{uncover_search_phrase}' -l 9999999 > data/{divd_case_number}/censys-results.txt"

    #TODO: Uncover provides -e flag with which it is possible to use multiple search engines at the same time. It should be 
    #TODO: checked whether it removes duplicate ips from the final reuslt by itself or if one big final ip address file
    #TODO: should be created manually (programmatically). Next, parse the txt file into csv file so that it fits into the 
    #TODO: nuclei command in the following task.

    uncover_command = f"uncover -e shodan,censys -s '{uncover_search_phrase}' -l 9999999 > data/{divd_case_number}/uncover-results.txt"


    # Execute the commands and handle errors
    try:
        #subprocess.run(shodan_command, shell=True, check=True)
        subprocess.run(uncover_command, shell=True, check=True)
    except CalledProcessError as e:
        print(f"An error occurred while running Shodan command: {e}")

    #try:
        #subprocess.run(censys_command, shell=True, check=True)
    #except CalledProcessError as e:
    #    print(f"An error occurred while running Censys command: {e}")
    
    print("Uncover scan completed.")


def run_shodan_scan(**kwargs):
    # Get Shodan search phrase and date from Airflow Variable
    shodan_search_phrase = Variable.get("shodan_search_phrase")
    shodan_api_key = Variable.get("shodan_api_key")
    divd_case_number = Variable.get("divd_case_number")  # Retrieve DIVD case number

    # Constructing the Shodan download command
    download_command = f"shodan download --limit -1 /opt/airflow/data/{divd_case_number}/shodan-download '{shodan_search_phrase}'"
    print(download_command)

    # Initialize Shodan with the API key - Variable needs to be set in Airflow UI
    init_command = f"shodan init {shodan_api_key}"
    try:
        subprocess.run(init_command, shell=True, check=True)
    except CalledProcessError as e:
        print(f"An error occurred while initializing Shodan: {e}")
        return

    # Execute the download command and handle errors
    try:
        subprocess.run(download_command, shell=True, check=True)
    except CalledProcessError as e:
        print(f"An error occurred while running Shodan download command: {e}")

    # Constructing the Shodan convert command to get ip_str and port in CSV format
    convert_command = f"shodan parse --fields ip_str,port --separator : 'data/{divd_case_number}/shodan-download.json.gz' > 'data/{divd_case_number}/shodan-results.csv'"

    # Execute the convert command and handle errors
    try:
        subprocess.run(convert_command, shell=True, check=True)
    except CalledProcessError as e:
        print(f"An error occurred while converting Shodan data: {e}")

    print("Shodan scan and conversion completed.")


def run_nuclei_scan(**kwargs):
    fingerprint_file_path = Variable.get("fingerprint_file_path")
    divd_case_number = Variable.get("divd_case_number")
    shodan_results_path = f"data/{divd_case_number}/shodan-results.csv"  # Path to the CSV file generated by the previous task
    
    # Nuclei command with parameters used by DIVD
    nuclei_command = (
        f"nuclei -H 'User-Agent: {divd_case_number}' -t {fingerprint_file_path} "
        f"-l {shodan_results_path} -retries 2 -timeout 6 "
        f"-output data/{divd_case_number}/nuclei_results.json -j"
    )

    # Execute the Nuclei command and handle errors
    try:
        subprocess.run(nuclei_command, shell=True, check=True)
    except CalledProcessError as e:
        print(f"An error occurred while running Nuclei command: {e}")
        print(e.output)
    
    print("Nuclei scan completed.")


def enrich_nuclei_data(**kwargs):
    # Retrieve the DIVD case number from Airflow Variables
    divd_case_number = Variable.get("divd_case_number")

    # File paths for input and output
    nuclei_output_path = f"data/{divd_case_number}/nuclei_results.json"
    enriched_results_path = f"data/{divd_case_number}/enriched_results.json"
    enriched_results_with_email_path = f"data/{divd_case_number}/enriched_results_with_email.json"
    enriched_results_without_email_path = f"data/{divd_case_number}/enriched_results_without_email.json"

    # Run the nuclei-parse-enrich tool
    cmd = f'cd /opt/airflow/nuclei-parse-enrich && go run cmd/main.go -i ../{nuclei_output_path} -o ../{enriched_results_path}'
    subprocess.run(cmd, shell=True, check=True)

    # Open and read the enriched results file
    with open(enriched_results_path, 'r') as file:
        enriched_data = json.load(file)

    # Initialize dictionaries for data with and without emails
    with_email_data = {}
    without_email_data = {}

    # Regex for detecting emails
    email_regex = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'

    # Iterate through each entry in the enriched data
    for ip, entry in enriched_data.items():
        # Check for security.txt information
        ip_or_domain = entry.get('ip') or entry.get('host')
        if ip_or_domain:
            try:
                response = requests.get(f'https://{ip_or_domain}/.well-known/security.txt')
                if response.status_code == 200:
                    emails = re.findall(email_regex, response.text)
                    entry['security_txt_email'] = emails[0] if emails else None
            except requests.exceptions.RequestException:
                pass

        # Separate data with and without email information
        abuse_email_present = 'Abuse' in entry and entry['Abuse'].strip()
        security_txt_email_present = 'security_txt_email' in entry and entry['security_txt_email'].strip()

        if abuse_email_present or security_txt_email_present:
            with_email_data[ip] = entry
        else:
            without_email_data[ip] = entry

    # Write the data with emails to a file
    with open(enriched_results_with_email_path, 'w') as file:
        json.dump(with_email_data, file, indent=4)

    # Write the data without emails to a file
    with open(enriched_results_without_email_path, 'w') as file:
        json.dump(without_email_data, file, indent=4)

    print("Enrichment step completed.")


default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 1, 1),
}


with DAG(
    dag_id='cvd_scan_and_enrich', 
    default_args=default_args, 
    schedule_interval=None,  # set to None for manual trigger
    description='A DAG for vulnerability scanning',
    catchup=False  # set to False - no backfilling
) as dag:

    run_shodan_scan_task = PythonOperator(
        task_id='run_shodan_scan_task',
        python_callable=run_shodan_scan,
        provide_context=True
    )

    run_nuclei_scan_task = PythonOperator(
        task_id='run_nuclei_scan_task',
        python_callable=run_nuclei_scan,
        provide_context=True
    )

    enrich_nuclei_data_task = PythonOperator(
        task_id='enrich_nuclei_data_task',
        python_callable=enrich_nuclei_data,
        provide_context=True
    )

    run_shodan_scan_task >> run_nuclei_scan_task >> enrich_nuclei_data_task
